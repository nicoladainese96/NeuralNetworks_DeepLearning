{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of some books by Jane Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mansfield_park.txt', 'sense_sensibility.txt', 'lady_susan.txt', 'persuasion.txt', 'pride_prejudice.txt', 'northanger_abbey.txt']\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Preprocessing Books/mansfield_park.txt \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Preprocessing Books/sense_sensibility.txt \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Preprocessing Books/lady_susan.txt \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Preprocessing Books/persuasion.txt \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Preprocessing (ad hoc) Books/pride_prejudice.txt\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Preprocessing Books/northanger_abbey.txt \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = prep.austen_preprocessing(debug=False)\n",
    "# dataset is a dictionary\n",
    "#dataset.keys()\n",
    "\n",
    "# I saved it just once, since every time the numerical encoding changes\n",
    "#np.save(\"dataset\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2index size:  12811\n",
      "Encoded name:  10608\n",
      "Retrieved name:  bingley\n",
      "Number of sentences in the text:  21898\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"dataset.npy\", allow_pickle=True).item()\n",
    "\n",
    "word2index = dataset['word2index']\n",
    "index2word = dataset['index2word']\n",
    "num_sentences = dataset['num_sentences']\n",
    "print(\"word2index size: \", len(word2index))\n",
    "\n",
    "num_w = word2index['bingley']\n",
    "retrieved = index2word[num_w]\n",
    "print(\"Encoded name: \", num_w)\n",
    "print(\"Retrieved name: \", retrieved)\n",
    "\n",
    "print(\"Number of sentences in the text: \", len(num_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AustenDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, transform=None):\n",
    "        \n",
    "        dataset = np.load(filepath, allow_pickle=True).item()\n",
    "        \n",
    "        self.word2index = dataset['word2index']\n",
    "        self.index2word = dataset['index2word']\n",
    "        self.num_sentences = dataset['num_sentences']\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        sorted_seq = sorted(self.num_sentences, key=lambda x: len(x), reverse=True)\n",
    "        # save original lengths of each sentence (subtract 1 for slicing in x and y)       \n",
    "        self.seq_len = [len(s) -1 for s in sorted_seq] \n",
    "        \n",
    "        enc_sentences = [torch.LongTensor(enc_s) for enc_s in sorted_seq]\n",
    "        \n",
    "        x = [s[:-1] for s in enc_sentences] # last word of each sentence doesn't have a label\n",
    "        y = [s[1:] for s in enc_sentences] # first word of each sentence doesn't have an x associated\n",
    "        \n",
    "        x_padded = pad_sequence(x, batch_first=True)\n",
    "        y_padded = pad_sequence(y, batch_first=True)\n",
    "        \n",
    "        seq_len = np.array(self.seq_len)\n",
    "        mask = (seq_len > 0)\n",
    "        \n",
    "        x_padded = x_padded[mask]\n",
    "        y_padded = y_padded[mask]\n",
    "        seq_len = torch.LongTensor(seq_len[mask])\n",
    "\n",
    "        self.x_padded = x_padded\n",
    "        self.y_padded = y_padded\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = len(self.word2index)\n",
    "          \n",
    "    def __len__(self):\n",
    "        return len(self.x_padded)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get text\n",
    "        x = self.x_padded[idx]\n",
    "        y = self.y_padded[idx]\n",
    "        length = self.seq_len[idx]\n",
    "        return (x, y, length)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train set:  553\n"
     ]
    }
   ],
   "source": [
    "full_dataset = AustenDataset(\"dataset.npy\")\n",
    "\n",
    "training_size = int(0.9 * len(full_dataset))\n",
    "test_size = len(full_dataset) - training_size\n",
    "train_size = int(0.9 * training_size)\n",
    "val_size = training_size - train_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_sampler = SubsetRandomSampler(np.arange(train_size))\n",
    "val_sampler = SubsetRandomSampler(np.arange(val_size))\n",
    "test_sampler = SubsetRandomSampler(np.arange(test_size))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=4, drop_last=True, sampler = train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 128, num_workers=4, drop_last=False, sampler = val_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 128, num_workers=4, drop_last=False, sampler = test_sampler)\n",
    "\n",
    "print(\"Number of batches in train set: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hidden_units, layers_num, dropout_prob=0, linear_size=512):\n",
    "        super().__init__()\n",
    "        # Define recurrent layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=emb_dim, \n",
    "                           hidden_size=hidden_units,\n",
    "                           num_layers=layers_num,\n",
    "                           dropout=dropout_prob,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.l1 = nn.Linear(hidden_units,linear_size)\n",
    "        self.out = nn.Linear(linear_size,vocab_size) # leave out the '<PAD>' label \n",
    "        \n",
    "    def forward(self, x, seq_lengths, state=None):\n",
    "\n",
    "        # Embedding of x\n",
    "        x = self.embedding(x) \n",
    "        \n",
    "        # packing for efficient processing\n",
    "        packed_input = pack_padded_sequence(x, seq_lengths, batch_first=True)\n",
    "        \n",
    "        # propagate through the LSTM\n",
    "        packed_output, state = self.rnn(packed_input, state)\n",
    "        \n",
    "        # unpack for linear layers processing\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        #print(\"Unpacked output: \", output.size(), '\\n')\n",
    "        \n",
    "        # Linear layer\n",
    "        output = F.leaky_relu(self.l1(output))\n",
    "\n",
    "        # Linear layer with log_softmax (useful for custom cross-entropy)\n",
    "        output = F.log_softmax(self.out(output), dim=2)\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_ent_loss(y_true, y_pred, pad_token=0, debug=False):\n",
    "    \"\"\"\n",
    "    y_true should be of size (batch_size, seq_len)\n",
    "    y_pred should be of size (batch_size, seq_len, vocab_size) \n",
    "    where vocab_size does not count the token '<PAD>' entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_t_flat = y_true.reshape(-1)\n",
    "    y_p_flat = y_pred.view(-1, y_pred.size()[-1])\n",
    "    if debug:\n",
    "        print(\"y_t_flat.size() : \", y_t_flat.size())\n",
    "        print(\"y_p_flat.size() : \", y_p_flat.size())\n",
    "        \n",
    "    mask = (y_t_flat>0).float() # consider only the non-padded words\n",
    "    \n",
    "    num_tokens = int(torch.sum(mask).item()) # check if .item() is good, otherwise use .data[0]\n",
    "    if debug:\n",
    "        print(\"Num tokens : \", num_tokens)\n",
    "        \n",
    "    # for each word choose the log of the prob predicted for the right label\n",
    "    y_p_masked = y_p_flat[range(y_p_flat.shape[0]), y_t_flat] * mask \n",
    "    \n",
    "    # compute cross entropy\n",
    "    cross_ent_loss = - torch.sum(y_p_masked) / num_tokens\n",
    "    if debug:\n",
    "        print(\"Cross-entropy : \", cross_ent_loss)\n",
    "        \n",
    "    return cross_ent_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(vocab_size=len(word2index), emb_dim=100, hidden_units=128, layers_num=2, dropout_prob=0.2)\n",
    "net = Network(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(net.parameters(), lr=0.1, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_one_epoch(net, optimizer, train_loader, val_loader, debug=False, verbose=True):\n",
    "    \n",
    "    verbose_print = print if verbose else lambda *args, **kwargs : None\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "        \n",
    "    n_batches = len(train_loader)\n",
    "    print_every = n_batches // 10\n",
    "    start_time = time.time()\n",
    "    \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        x, y, lengths = data\n",
    "        sorted_x = torch.LongTensor(sorted(x.numpy(), key=lambda x: np.count_nonzero(x), reverse=True))\n",
    "        sorted_y = torch.LongTensor(sorted(y.numpy(), key=lambda x: np.count_nonzero(x), reverse=True))\n",
    "        sorted_lengths = torch.LongTensor(sorted(lengths.numpy(), key=lambda x: x, reverse=True))\n",
    "\n",
    "        L_max = sorted_lengths.max().item()\n",
    "        y_trunc = sorted_y[:,:L_max]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            output, _ = net(sorted_x,sorted_lengths) # returns (output, state)\n",
    "        except RuntimeError:\n",
    "            print(\"Lengths: \", sorted_lengths)\n",
    "            \n",
    "        loss = masked_cross_ent_loss(y_trunc, output, pad_token=0, debug=debug)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if ((i+1) % (print_every) == 0) or (i == n_batches - 1):\n",
    "                verbose_print('\\r'+\"Batch {}/{}, {:d}% \\t Train loss: {:.3f} took: {:.2f}s \".format(\n",
    "                        i+1, n_batches, int(100 * (i+1) / n_batches), train_loss / (i+1),\n",
    "                        time.time() - start_time), end=' ')\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader,0):\n",
    "\n",
    "            x, y, lengths = data\n",
    "            sorted_x = torch.LongTensor(sorted(x.numpy(), key=lambda x: np.count_nonzero(x), reverse=True))\n",
    "            sorted_y = torch.LongTensor(sorted(y.numpy(), key=lambda x: np.count_nonzero(x), reverse=True))\n",
    "            sorted_lengths = torch.LongTensor(sorted(lengths.numpy(), key=lambda x: x, reverse=True))\n",
    "\n",
    "            L_max = sorted_lengths.max().item()\n",
    "            y_trunc = sorted_y[:,:L_max]\n",
    "\n",
    "            output, _ = net(sorted_x, sorted_lengths) # returns (output, state)\n",
    "            loss = masked_cross_ent_loss(y_trunc, output, pad_token=0, debug=debug)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return net, optimizer, train_loss/len(train_loader), val_loss/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 553/553, 100% \t Train loss: 5.903 took: 2590.44s  \n",
      "Epoch: 1 - time: 2718.21 - train loss: 5.9026 - val loss: 5.4374\n",
      "Batch 553/553, 100% \t Train loss: 5.296 took: 2572.29s  \n",
      "Epoch: 2 - time: 2703.51 - train loss: 5.2963 - val loss: 5.1771\n",
      "Batch 553/553, 100% \t Train loss: 5.140 took: 2611.81s  \n",
      "Epoch: 3 - time: 2740.92 - train loss: 5.1401 - val loss: 5.0704\n",
      "Batch 553/553, 100% \t Train loss: 5.052 took: 2616.86s  \n",
      "Epoch: 4 - time: 2738.02 - train loss: 5.0517 - val loss: 5.0309\n",
      "Batch 553/553, 100% \t Train loss: 5.017 took: 2613.74s  \n",
      "Epoch: 5 - time: 2741.10 - train loss: 5.0170 - val loss: 5.0114\n",
      "Batch 553/553, 100% \t Train loss: 5.001 took: 2599.53s  \n",
      "Epoch: 6 - time: 2729.39 - train loss: 5.0011 - val loss: 4.9891\n",
      "Batch 553/553, 100% \t Train loss: 4.990 took: 2588.18s  \n",
      "Epoch: 7 - time: 2715.93 - train loss: 4.9905 - val loss: 4.9924\n",
      "Batch 553/553, 100% \t Train loss: 4.983 took: 2591.37s  \n",
      "Epoch: 8 - time: 2726.06 - train loss: 4.9832 - val loss: 4.9748\n",
      "Batch 553/553, 100% \t Train loss: 4.975 took: 2680.11s  \n",
      "Epoch: 9 - time: 2810.07 - train loss: 4.9754 - val loss: 4.9635\n",
      "Batch 553/553, 100% \t Train loss: 4.971 took: 2602.75s  \n",
      "Epoch: 10 - time: 2733.83 - train loss: 4.9713 - val loss: 4.9682\n",
      "Batch 553/553, 100% \t Train loss: 4.970 took: 2582.44s  \n",
      "Epoch: 11 - time: 2710.30 - train loss: 4.9705 - val loss: 4.9585\n",
      "Batch 553/553, 100% \t Train loss: 4.966 took: 2582.02s  \n",
      "Epoch: 12 - time: 2712.95 - train loss: 4.9659 - val loss: 4.9699\n",
      "Batch 553/553, 100% \t Train loss: 4.964 took: 2600.27s  \n",
      "Epoch: 13 - time: 2729.95 - train loss: 4.9639 - val loss: 4.9605\n",
      "Batch 553/553, 100% \t Train loss: 4.964 took: 2607.29s  \n",
      "Epoch: 14 - time: 2737.69 - train loss: 4.9636 - val loss: 4.9611\n",
      "Batch 553/553, 100% \t Train loss: 4.960 took: 2618.40s  \n",
      "Epoch: 15 - time: 2751.12 - train loss: 4.9600 - val loss: 4.9542\n",
      "Batch 553/553, 100% \t Train loss: 4.958 took: 2585.24s  \n",
      "Epoch: 16 - time: 2711.17 - train loss: 4.9577 - val loss: 4.9559\n",
      "Batch 553/553, 100% \t Train loss: 4.954 took: 2614.69s  \n",
      "Epoch: 17 - time: 2742.56 - train loss: 4.9542 - val loss: 4.9658\n",
      "Batch 553/553, 100% \t Train loss: 4.953 took: 2622.09s  \n",
      "Epoch: 18 - time: 2753.17 - train loss: 4.9532 - val loss: 4.9503\n",
      "Batch 553/553, 100% \t Train loss: 4.954 took: 2616.92s  \n",
      "Epoch: 19 - time: 2749.93 - train loss: 4.9535 - val loss: 4.9400\n",
      "Batch 553/553, 100% \t Train loss: 4.951 took: 2596.15s  \n",
      "Epoch: 20 - time: 2726.31 - train loss: 4.9511 - val loss: 4.9282\n"
     ]
    }
   ],
   "source": [
    "# first cycle of training\n",
    "n_epochs = 20\n",
    "train_log = []\n",
    "val_log = []\n",
    "for e in range(n_epochs):    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    net, optimizer, train_loss, val_loss = train_one_epoch(net, optimizer, train_loader, val_loader, debug=False)\n",
    "    epoch_time = time.time() - start_time\n",
    "    torch.save(net.state_dict(), 'params'+str(e)+'.pth')\n",
    "    train_log.append(train_loss)\n",
    "    val_log.append(val_loss)\n",
    "    print(\"\\nEpoch: %d - time: %.2f - train loss: %.4f - val loss: %.4f\"%((e+1), epoch_time, train_loss, val_loss))\n",
    "torch.save(net.state_dict(), 'final_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = list(np.load(\"train_log.npy\"))\n",
    "val_log = list(np.load(\"val_log.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(np.array(train_log), label = 'train loss')\n",
    "plt.plot(np.array(val_log), label='val loss')\n",
    "plt.xlabel('Number of epochs', fontsize=16)\n",
    "plt.ylabel('Cross-entropy loss', fontsize=16)\n",
    "plt.xticks(np.arange(0,5)*5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_log\", train_log)\n",
    "np.save(\"val_log\", val_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_net = Network(**params)\n",
    "trained_net.load_state_dict(torch.load('final_params.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"You are mr. Darcy I suppose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_v0(seed, trained_net, word2index, index2word, len_generated_seq = 10, debug=False, T=1):\n",
    "    \n",
    "    # preprocess seed\n",
    "    import re\n",
    "    special_chars = ['!','?','&','(',')','*','-','_',':',';','\"','\\'','1','2','3','4','5','6','7','8','9','0']\n",
    "    for x in special_chars:\n",
    "        seed = seed.replace(x,' ')\n",
    "    full_text = seed.lower()\n",
    "    full_text = full_text.replace('mr.','mr')\n",
    "    full_text = full_text.replace('mrs.','mrs')\n",
    "    full_text = re.sub('à', 'a', full_text)\n",
    "    full_text = re.sub('ê', 'e', full_text)\n",
    "    full_text = re.sub(r'[.]',' .\\n', full_text)\n",
    "    full_text = full_text.replace(',',' ,')\n",
    "    full_text = full_text.replace('  ',' ')\n",
    "    \n",
    "    num_sentence = prep.numerical_encoder(full_text, word2index)\n",
    "    if debug:\n",
    "        for i,num_w in enumerate(num_sentence):\n",
    "            print(i,num_w)\n",
    "        \n",
    "    enc_sentence = torch.LongTensor(num_sentence).view(1,-1)\n",
    "    context = enc_sentence[:,:-1]\n",
    "    length_context = torch.LongTensor(np.array([len(num_sentence)-1])).view(1)\n",
    "    last_word = enc_sentence[:,-1].view(1,1)\n",
    "    length_last = torch.LongTensor([1]).view(1)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"enc_sentence : \", enc_sentence.size())\n",
    "        print(\"context : \", context.size(), context)\n",
    "        print(\"length_context: \", length_context.size(), length_context)\n",
    "        print(\"last_word: \", last_word.size(), last_word)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        _, hidden_context = net(context, length_context)\n",
    "\n",
    "        gen_words = []\n",
    "        for i in range(len_generated_seq):\n",
    "            last_word_ohe, hidden_context = net(last_word, length_last, state=hidden_context)\n",
    "            prob_last_word = np.exp(last_word_ohe.numpy().flatten()/T)\n",
    "            prob_last_word = prob_last_word/ prob_last_word.sum()\n",
    "            \n",
    "            if debug:\n",
    "                print(\"prob_last_word (shape): \", prob_last_word.shape)\n",
    "                print(\"Sum of probabilities: \", prob_last_word.sum())\n",
    "                print(\"'<PAD>' probability: \", prob_last_word[0])\n",
    "            last_word_np = np.random.choice(np.arange(len(prob_last_word)), p=prob_last_word)\n",
    "            gen_words.append(last_word_np)\n",
    "            last_word = torch.LongTensor([last_word_np]).view(1,1)\n",
    "            if debug:\n",
    "                print(\"Last word: \", last_word_np)\n",
    "    \n",
    "    gen_words = np.array(gen_words).flatten()\n",
    "    decoded_sentence = prep.numerical_decoder(gen_words, index2word)\n",
    "    output_string = ' '.join(decoded_sentence)\n",
    "    print(\"Seed: \", seed, '\\n')\n",
    "    print(\"Generated sentence: \", output_string)\n",
    "    print(\"\\nAll toghether: \", seed, output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  You are mr. Darcy I suppose \n",
      "\n",
      "Generated sentence:  you feelings it will be said just s there time , time is nothing from arrear . about . of occupies from i were required i am sorry you . to convince none but i do not keep them , however , by my sisters , never kept feel any chance him our way of interest black on this morning as tom carteret to let care at home from her as when the best better they are very congratulated , replied the time , worth concurrence into tambour . , which you know of the china that seizing , likes\n",
      "\n",
      "All toghether:  You are mr. Darcy I suppose you feelings it will be said just s there time , time is nothing from arrear . about . of occupies from i were required i am sorry you . to convince none but i do not keep them , however , by my sisters , never kept feel any chance him our way of interest black on this morning as tom carteret to let care at home from her as when the best better they are very congratulated , replied the time , worth concurrence into tambour . , which you know of the china that seizing , likes\n"
     ]
    }
   ],
   "source": [
    "generate_sentence_v0(seed, trained_net, word2index, index2word, len_generated_seq=100, debug=False, T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
